from llama_cpp import Llama
from colorama import Fore, Style, init
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader
from langchain.text_splitter import CharacterTextSplitter
from huggingface_hub import hf_hub_download
import argparse
import sys
import os
import shutil

if getattr(sys, 'frozen', False):
    # PyInstallerã§ãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸå®Ÿè¡Œç’°å¢ƒ
    SCRIPT_DIR_PATH = os.path.dirname(sys.executable)
else:
    # ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
    SCRIPT_DIR_PATH = os.path.dirname(os.path.abspath(__file__))

# HuggingFaceä¸Šã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹embeddingãƒ¢ãƒ‡ãƒ«å
EMBEDDING_MODEL_PATH = "intfloat/multilingual-e5-large-instruct"

# HuggingFaceä¸Šã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹LLMã®ãƒªãƒã‚¸ãƒˆãƒªå
HF_REPO_NAME = "lmstudio-community/gemma-3-1B-it-qat-GGUF"

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã„LLMã®ggufãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå¤§æ–‡å­—ãƒ»å°æ–‡å­—ã‚’æ­£ç¢ºã«ï¼‰
GGUF_FILE_NAME = "gemma-3-1B-it-QAT-Q4_0.gguf"

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸ GGUF ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
MODEL_PATH = os.path.join(SCRIPT_DIR_PATH, "models", GGUF_FILE_NAME)

if os.path.exists(MODEL_PATH):
    # Llama ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç”Ÿæˆ
    llm = Llama( # ã“ã‚ŒãƒŸã‚¹ã€‚æˆ»ã›
        model_path=MODEL_PATH,
        n_ctx=12000,         # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        n_threads=4,        # ä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
        n_gpu_layers=-1,
        verbose=False
    )
else:
    print(f"{GGUF_FILE_NAME} is not found.")
    
# ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ãŸã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
def load_documents(file_path: str):
    if file_path.lower().endswith(".pdf"):
        loader = PyPDFLoader(file_path)
    elif file_path.lower().endswith(".txt"):
        loader = TextLoader(file_path, encoding="utf-8")
    elif file_path.lower().endswith(".csv"):
        loader = CSVLoader(file_path, encoding="utf-8")
    else:
        raise ValueError(f"Unsupported file type: {file_path}")
    return loader.load()

# ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯å˜ä½ã«åˆ†å‰²
def split_documents(documents, chunk_size=1000, chunk_overlap=0.1):
    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_documents(documents)

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã‚¹ãƒˆã‚¢ã™ã‚‹
def embed_and_store(
    raw_docs,
    model_name=EMBEDDING_MODEL_PATH,
    device="cpu",
    persist_path="faiss_index"
):
    # ã‚¹ãƒ†ãƒƒãƒ—1: ç”Ÿãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ¤œè¨¼
    if not raw_docs:
        raise ValueError("No documents to embed. Check loader output.")
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
    docs = split_documents(raw_docs, 100)
    print(raw_docs)
    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    print(f"Generated {len(docs)} chunks")
    # ã‚¹ãƒ†ãƒƒãƒ—3: ç©ºãƒãƒ£ãƒ³ã‚¯ã®é™¤å¤–
    docs = [doc for doc in docs if doc.page_content.strip()]
    if not docs:
        raise ValueError("All chunks are empty after filtering.")
    # ã‚¹ãƒ†ãƒƒãƒ—4: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": device},
    )
    # ã‚¹ãƒ†ãƒƒãƒ—5: FAISS ã«æ ¼ç´
    vectorstore = FAISS.from_documents(docs, embeddings)
    vectorstore.save_local(persist_path)
    return vectorstore

def update_vector():
    import glob

    # ä¾‹ãˆã°ã€Œdata/sample.txtã€ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ãŸã„å ´åˆ
    data_path = os.path.join(SCRIPT_DIR_PATH, 'data')
    file_paths = glob.glob(data_path+"/*.*")
    raw_docs = []
    for path in file_paths:
        raw_docs.extend(load_documents(path))
    # ä¿®æ­£ç‰ˆé–¢æ•°ã®å‘¼ã³å‡ºã—
    faiss_store = embed_and_store(
        raw_docs,
        model_name=EMBEDDING_MODEL_PATH,
        device="cpu",
        persist_path="faiss_index"
    )
    print(f"Indexed into FAISS at 'faiss_index'")


# ã‚ã‚‰ã‹ã˜ã‚ä½œã£ã¦ãŠã„ãŸãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ãƒ‰
def load_vectorstore(persist_path="faiss_index",
                     model_name="intfloat/multilingual-e5-large-instruct",
                     device="cpu"):
    """ä¿å­˜æ¸ˆã¿FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": device},
    )
    vectorstore = FAISS.load_local(
        persist_path,
        embeddings,
        allow_dangerous_deserialization=True
    )
    return vectorstore


def search_faiss(query: str, k: int = 5):
    """FAISSã§ã‚¯ã‚¨ãƒªæ¤œç´¢"""
    persist_path = "faiss_index"
    vectorstore = load_vectorstore(persist_path)

    print(f"ğŸ” Searching for: {query}")
    results = vectorstore.similarity_search(query, k=k)

    return results

def chat(prompt: str,
         max_tokens: int = 2048,
         temperature: float = 0.8,
         top_p: float = 0.95,):
    """
    promptï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’æ¸¡ã—ã¦ LLM ã§å¿œç­”ã‚’ç”Ÿæˆã—ã€
    ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    out = llm(
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        min_p=0.05,
        top_k=40,
        repeat_penalty=1.1,
        stop=["\n\n\n\n"]  # å¿…è¦ã«å¿œã˜ã¦ã‚¹ãƒˆãƒƒãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å¤‰æ›´
    )
    print(out)
    return out["choices"][0]["text"].strip()

def run():
    """
    ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®FAQã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    """
    init()
    query = input("ğŸ’¬ Query: ")
    search_result = search_faiss(query, k=2)
    search_result_str = ""
    for i, doc in enumerate(search_result, 1):
        search_result_str = search_result_str + f"\nResult #{i}"
        search_result_str = search_result_str + doc.page_content
        search_result_str = search_result_str + f"[Metadata] {doc.metadata}"

    prompt = f"### æŒ‡ç¤º \nã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆAIã§ã™ã€‚å¸¸ã«æ—¥æœ¬èªã§å¿œç­”ã—ã¾ã™ã€‚è³ªå•ã€Œ{query}ã€ã«ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚ãã®éš›ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n### æƒ…å ± \n{search_result_str}\n\n\n\n"
    response = Fore.GREEN + chat(prompt) + Style.RESET_ALL
    print("ğŸ¤–Gemma:", response)

def setup():
    """
    å¿…è¦ãªLLMã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """

    # å¿…è¦ãªã‚‰ revision ã‚’æŒ‡å®šï¼ˆä¾‹: "main"ï¼‰
    revision = "main"

    # èªè¨¼æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã†å ´åˆ
    # - ç’°å¢ƒå¤‰æ•° HF_TOKEN ã‚’è¨­å®šæ¸ˆã¿ãªã‚‰ use_auth_token=True
    # - ç›´æ¥æ–‡å­—åˆ—ã‚’æ¸¡ã™å ´åˆã¯ use_auth_token="hf_xxx..."
    path_to_hf_model_cache = hf_hub_download(
        repo_id=HF_REPO_NAME,
        filename=GGUF_FILE_NAME,
        revision=revision,
        use_auth_token=True,
    )
    print(path_to_hf_model_cache)

    path_to_dst_model_dir = os.path.join(SCRIPT_DIR_PATH, "models")
    os.makedirs(path_to_dst_model_dir, exist_ok=True)

    print(f"Moving: {path_to_hf_model_cache} -> {path_to_dst_model_dir}")
    shutil.move(path_to_hf_model_cache, path_to_dst_model_dir)

def main():
    parser = argparse.ArgumentParser(description="Local LLM oneâ€‘shot Q&A with semantic vector retrieval.")
    subparsers = parser.add_subparsers(dest='command', required=True)

    # run ã‚³ãƒãƒ³ãƒ‰
    subparsers.add_parser('run', help='1å•1ç­”å¼ã®å¯¾è©±ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚Queryã«å…¥åŠ›ã—ãŸå†…å®¹ã‚’å…ƒã«ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’è¡Œã„ã€ã“ã‚Œã«ã¤ã„ã¦LLMãŒè¦ç´„ã—ã¦å›ç­”ã—ã¾ã™ã€‚modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç›®çš„ã®.ggufãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„')

    # update-vector ã‚³ãƒãƒ³ãƒ‰
    subparsers.add_parser('update-vector', help='ãƒ™ã‚¯ãƒˆãƒ«ã®æ›´æ–°å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«csv, pdf, txtãªã©ã‚’é…ç½®ã—ã¦ãã ã•ã„')

    # setup ã‚³ãƒãƒ³ãƒ‰
    subparsers.add_parser('setup', help='ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’è¡Œã„ã¾ã™')

    # å¼•æ•°ãŒãªã„å ´åˆã¯ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args()
    print(args)

    if args.command == 'run':
        run()
    elif args.command == 'update-vector':
        update_vector()
    elif args.command == 'setup':
        setup()


if __name__ == "__main__":
    main()